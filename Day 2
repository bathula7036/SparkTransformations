%sql
1.Find duplicate records in a table (Amazon)

select customer_id, order_date, count(*) AS cnt
from orders
group by customer_id, order_date
having count(*) >1;

2.Retrieve the second highest salary from Employee table ()

select max(salary) AS SecondHighestSalary
from employee
where salary < (select max(salary) from employee);

3.Find employees without department (Uber)
select employee_id from employees
where department is null;

4.Find employees without department
select e.* 
from eployee
left join department d ON e.department_id = d.departmnet_id
where d.departmnet_id is null;

5.Calculate the total revenue per product (PayPal)
select product_id, SUM(quantity * price) as total_revenue
from Sales
GROUP BY product_id;


6.Get the top 3 highest-paid employees (Google)
select TOP 3 *
from employee
Group by salary desc;

7.Customers who made purchases but never returned products (Walmart)
select distinct c.customer_id
from customers c
join orders o ON c.customer_id = o.customer_id
where c.customer_id not in (select customer_id from Returns):



%Python
1.find dublicate value and sort in accending order

mylist = [2,5,7,5,3,4,5,2]

def find_duplicates_sorted(myList):
    seen = set()
    duplicates = set()

    for i in myList:
        if i in seen:
            duplicates.add(i)
        else:
            seen.add(i)

    return sorted(duplicates)

find_duplicates_sorted(mylist)


2. Sorting elements in the list

my_list = [5,8,9,6,3]

# method-1
for i in range(len(my_list)):

    min_index = i
    for j in range(i+1, len(my_list)):
        if my_list[j] < my_list[min_index]:
            my_list[i], my_list[j] = my_list[j], my_list[i]
print(my_list)
 

# method-2
my_list = [2,5,7,5,3,4,5,2]
ass = []

while my_list:
    minimum = my_list[0]
    for i in my_list:
        if i < minimum:
            minimum = i
    ass.append(minimum)
    my_list.remove(minimum)
print(ass)


%Pyspark
# Write a PySpark job to join two large datasets with skew handling.

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, rand, lit, when
import getpass

username = getpass.getuser()
spark = SparkSession.builder \
    .config('spark.ui.port', '0') \
    .config("spark.sql.warehouse.dir", f"/user/{username}/warehouse") \
    .enableHiveSupport() \
    .master('yarn') \
    .getOrCreate()

# Load Datasets
orders = spark.read.parquet("s3://path/to/orders")
customers = spark.read.parquet("s3://path/to/customers")

# Step 1: Identify skewed keys
skew_threshold = 10000
skewed_keys = orders.groupBy("customer_id") \
                    .count() \
                    .filter(col("count") > skew_threshold) \
                    .select("customer_id") \
                    .rdd.flatMap(lambda x: x) \
                    .collect()

# Step 2: Salt the skewed keys in orders
def salt_orders(df, skewed_keys, n_salt=10):
    salted_df = df.withColumn(
        "salted_key",
        when(
            col("customer_id").isin(skewed_keys),
            col("customer_id").cast("string") + "_" + (rand() * n_salt).cast("int").cast("string")
        ).otherwise(col("customer_id").cast("string"))
    )
    return salted_df

orders_salted = salt_orders(orders, skewed_keys)
customers_salted = customers.withColumn("salted_key", col("customer_id").cast("string"))

# Step 3: Join on salted key
joined_df = orders_salted.join(customers_salted, "salted_key", "inner").drop("salted_key")

# Step 4: Write results
joined_df.write.mode("overwrite").parquet("s3://path/to/joined_result")

